{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CISC 684 HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/secant78/Naive-Bayes-and-Logistic-Regression/blob/main/CISC_684_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl9CCR3oBf0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014aa683-0500-465f-cb5c-80f1addcdc10"
      },
      "source": [
        "\n",
        "#TODO: implement perceptron\n",
        "#MCAT Logistic algorithm\n",
        "#Naive Bayes - done\n",
        "#import training and test set - done\n",
        "\n",
        "import os\n",
        "import math\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#\"/content/drive/My Drive/\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx3kTJOkDGda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4b2529-7e35-4d85-bb36-feee7db21e83"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCu35thNBsdb"
      },
      "source": [
        "#import data set to list\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# get data file names\n",
        "pathHam =r'/content/drive/My Drive/CISC 684/dataset 1/train/ham'\n",
        "pathSpam = r'/content/drive/My Drive/CISC 684/dataset 1/train/spam'\n",
        "\n",
        "pathHamData2 = r'/content/drive/My Drive/CISC 684/dataset 2/train/ham'\n",
        "pathSpamData2 = r'/content/drive/My Drive/CISC 684/dataset 2/train/spam'\n",
        "\n",
        "\n",
        "pathHamData3 = r'/content/drive/My Drive/CISC 684/dataset 3/train/ham'\n",
        "pathSpamData3 = r'/content/drive/My Drive/CISC 684/dataset 3/train/spam'\n",
        "\n",
        "\n",
        "pathTestHam = r'/content/drive/My Drive/CISC 684/dataset 1/test/ham'\n",
        "pathTestSpam = r'/content/drive/My Drive/CISC 684/dataset 1/test/spam'\n",
        "\n",
        "\n",
        "pathTestHamData2 = r'/content/drive/My Drive/CISC 684/dataset 2/test/ham'\n",
        "pathTestSpamData2 = r'/content/drive/My Drive/CISC 684/dataset 2/test/spam'\n",
        "\n",
        "\n",
        "pathTestHamData3 = r'/content/drive/My Drive/CISC 684/dataset 3/test/ham'\n",
        "pathTestSpamData3 = r'/content/drive/My Drive/CISC 684/dataset 3/test/spam'\n",
        "\n",
        "\n",
        "filenamesHam = glob.glob(pathHam + \"/*.txt\")\n",
        "filenamesSpam = glob.glob(pathSpam + \"/*.txt\")\n",
        "\n",
        "\n",
        "filenamesHamData2 = glob.glob(pathHamData2 + \"/*.txt\")\n",
        "filenamesSpamData2 = glob.glob(pathSpamData2 + \"/*.txt\")\n",
        "\n",
        "\n",
        "filenamesHamData3 = glob.glob(pathHamData3 + \"/*.txt\")\n",
        "filenamesSpamData3 = glob.glob(pathSpamData3 + \"/*.txt\")\n",
        "\n",
        "\n",
        "filenameTestHam = glob.glob(pathTestHam + \"/*.txt\")\n",
        "filenameTestSpam = glob.glob(pathTestSpam + \"/*.txt\")\n",
        "\n",
        "\n",
        "filenameTestHamData2 = glob.glob(pathTestHamData2 + \"/*.txt\")\n",
        "filenameTestSpamData2 = glob.glob(pathTestSpamData2 + \"/*.txt\")\n",
        "\n",
        "\n",
        "filenameTestHamData3 = glob.glob(pathTestHamData2 + \"/*.txt\")\n",
        "filenameTestSpamData3 = glob.glob(pathTestSpamData2 + \"/*.txt\")\n",
        "\n",
        "dfsHam = []\n",
        "dfsSpam = []\n",
        "\n",
        "\n",
        "dfsHamData2 = []\n",
        "dfsSpamData2 = []\n",
        "\n",
        "dfsHamData3 = []\n",
        "dfsSpamData3 = []\n",
        "\n",
        "dfsTestHam = []\n",
        "dfsTestSpam = []\n",
        "\n",
        "dfsTestHamData2 = []\n",
        "dfsTestSpamData2 = []\n",
        "\n",
        "dfsTestHamData3 = []\n",
        "dfsTestSpamData3 = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t49MoYacLp0h"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok229OYcFU8B"
      },
      "source": [
        "#adds all file lines into an array\n",
        "def readFile(fileName):\n",
        "        fileObj = open(fileName, \"r\", encoding=\"ISO-8859-1\") #opens the file in read mode\n",
        "        words = fileObj.read() #puts the file into an array\n",
        "        #words['a'] = words['a'].str.join(\" \") \n",
        "        #print(words)\n",
        "        #words = unicode(words, errors='ignore')\n",
        "        fileObj.close()\n",
        "        words = words.translate(str.maketrans('', '', string.punctuation))    # Remove punctuation\n",
        "        tokens = nltk.word_tokenize(words)\n",
        "\n",
        "        #stopWords = set(stopwords.words('english'))         # Get list of stop words\n",
        "        terms = []\n",
        "        for word in tokens:\n",
        "          if word.isalpha():\n",
        "            word = word.lower()                         # Convert word to lower case\n",
        "            word = wn.morphy(word)                      # Convert word to its stem\n",
        "            terms.append(word)                          # Add stemmed word to words\n",
        "        return terms\n",
        "\n",
        "       # return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iG0bhqfZcqk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "aa5b936e-8d82-46bb-d953-ed625c886d09"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GIx8wbLGquC"
      },
      "source": [
        "def tokenizeFiles(dfs, filenames):\n",
        "  for filename in filenames:\n",
        "    dfs.append(readFile(filename))  \n",
        "  return dfs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk2RjxQatl5J"
      },
      "source": [
        "dfsHam = tokenizeFiles(dfsHam,filenamesHam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BjGkt0BdNb1"
      },
      "source": [
        "dfsSpam = tokenizeFiles(dfsSpam,filenamesSpam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9RlM-6-GvIw"
      },
      "source": [
        "dfsHamData2 = tokenizeFiles(dfsHamData2,filenamesHamData2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zno2ws_8Gv3D"
      },
      "source": [
        "dfsSpamData2 = tokenizeFiles(dfsSpamData2,filenamesSpamData2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq5fOq_lGw_8"
      },
      "source": [
        "dfsHamData3 = tokenizeFiles(dfsHamData3,filenamesHamData3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKS0uzFyGxb6"
      },
      "source": [
        "dfsSpamData3 = tokenizeFiles(dfsSpamData3,filenamesSpamData3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzjlD6cgHDCq"
      },
      "source": [
        "dfsTestHam = tokenizeFiles(dfsTestHam,filenameTestHam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-HixwwudOCF"
      },
      "source": [
        "dfsTestSpam = tokenizeFiles(dfsTestSpam, filenameTestSpam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrqh0jSSHIcD"
      },
      "source": [
        "dfsTestHamData2 = tokenizeFiles(dfsTestHamData2,filenameTestHamData2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5g_tH5EHI-n"
      },
      "source": [
        "dfsTestSpamData2 = tokenizeFiles(dfsTestSpamData2, filenameTestSpamData2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYxzzqTsHH_8"
      },
      "source": [
        "dfsTestHamData3 = tokenizeFiles(dfsTestHamData3,filenameTestHamData3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LssjMqUFHHp3"
      },
      "source": [
        "dfsTestSpamData3 = tokenizeFiles(dfsTestSpamData3, filenameTestSpamData3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8lq5e8vxDjF"
      },
      "source": [
        "MultiNomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vjnw5qSHuFj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "51cd6850-689c-48d1-b294-44418f8b691f"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "#makes a dictionary of word counts in file\n",
        "def wordListToFreqDict(wordlist):\n",
        "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
        "    return dict(list(zip(wordlist,wordfreq)))\n",
        "\n",
        "\n",
        "\n",
        "#takes lists of all words in training set and passes them to wordListToFreqDict()\n",
        "#which converts the words into word frequencies. Then it combines the word frequencies\n",
        "#of each file into one dictionary called setTerms which has all unique keys with word counts\n",
        "#of all files\n",
        "\n",
        "def getWords(dfs):\n",
        "\n",
        "  terms = {}\n",
        "  setTerms = {}\n",
        "  count = 0\n",
        "\n",
        "  for file in dfs:\n",
        "    terms[str(count)] = wordListToFreqDict(file)\n",
        "    count +=1\n",
        "  terms = dict((k, v) for k, v in terms.items() if k is not None)\n",
        "  for filewords in terms:\n",
        "    for word in terms[filewords]:\n",
        "      if word in setTerms:\n",
        "        setTerms[word]+= terms.get(filewords, {}).get(word)\n",
        "      else:\n",
        "        setTerms[word] = terms.get(filewords, {}).get(word)\n",
        "  final_dict = dict((k, v) for k, v in setTerms.items() if k is not None)\n",
        "  return final_dict\n",
        "\n",
        "def getWords2(array):\n",
        "  terms = {}\n",
        "  setTerms = {}\n",
        "  terms['0'] = wordListToFreqDict(array)\n",
        "  for filewords in terms:\n",
        "    for word in terms[filewords]:\n",
        "      if word in setTerms:\n",
        "        setTerms[word]+= terms.get(filewords, {}).get(word)\n",
        "      else:\n",
        "        setTerms[word] = terms.get(filewords, {}).get(word)\n",
        "  return setTerms\n",
        "\n",
        "\n",
        "#data strutures for doing the analysis / algorithms\n",
        "termsHam = getWords(dfsHam)\n",
        "#print(termsHam)\n",
        "termsSpam = getWords(dfsSpam)\n",
        "\n",
        "termsHamData2 = getWords(dfsHamData2)\n",
        "termsSpamData2 = getWords(dfsSpamData2)\n",
        "\n",
        "termsHamData3 = getWords(dfsHamData3)\n",
        "termsSpamData3 = getWords(dfsSpamData3)\n",
        "\n",
        "termsTestHam = getWords(dfsTestHam)\n",
        "termsTestSpam = getWords(dfsTestSpam)\n",
        "#print(termsHam)\n",
        "#print(termsSpam)\n",
        "\n",
        "\n",
        "\n",
        "#calculate fraction of ham and spam in entire data set and their probabilities\n",
        "nHam = len(dfsHam)          # Find the number of ham files there are in the training data\n",
        "nSpam = len(dfsSpam)        # Find the number of spam files there are in the training data\n",
        "pHam = nHam / (nHam + nSpam)        # Find the probability that a file is of class 'ham'\n",
        "pSpam = nSpam / (nHam + nSpam)\n",
        "\n",
        "\n",
        "def initial_prob(dfsHam,dfsSpam):\n",
        "  nHam = len(dfsHam)         \n",
        "  nSpam = len(dfsSpam)        \n",
        "  pHam = nHam / (nHam + nSpam)        \n",
        "  pSpam = nSpam / (nHam + nSpam)\n",
        "\n",
        "  return pHam\n",
        "\n",
        "probHamData2 = initial_prob(dfsHamData2,dfsSpamData2)\n",
        "probSpamData2 = 1-probHamData2\n",
        "probHamData2 = math.log10(probHamData2)\n",
        "probSpamData2 = math.log10(probSpamData2)\n",
        "\n",
        "\n",
        "probHamData3 = initial_prob(dfsHamData3,dfsSpamData3)\n",
        "probSpamData3 = 1-probHamData3\n",
        "probHamData3 = math.log10(probHamData3)\n",
        "probSpamData3 = math.log10(probSpamData3)\n",
        "\n",
        "\n",
        "probabilityHam = math.log10(pHam)\n",
        "probabilitySpam = math.log10(pSpam)\n",
        "\n",
        "\n",
        "\n",
        "#calculates the denominator of term probability formula. passed onto termProbability function\n",
        "def getTermDenominator(termdict):\n",
        "  wordCount = sum(termdict.values())\n",
        "  m = len(termdict)\n",
        "  return wordCount + m\n",
        "\n",
        "#calculates term probability based on term and term dictionary and getTermDenominator function\n",
        "def termProbability(term,termdict):\n",
        "  denominator = getTermDenominator(termdict)\n",
        "  termCount = 0\n",
        "  if term in termdict:\n",
        "    termCount += termdict[term]\n",
        "  else:\n",
        "    termCount = 0 \n",
        "  probability = (termCount + 1)/(denominator)\n",
        "  return probability\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def probabilityClassification(test_data, probClass, termDictionary):\n",
        "  \n",
        "\n",
        "  termprobabilities = []\n",
        "  for term in test_data:\n",
        "        termprobabilities.append(math.log10(termProbability(term, termDictionary)))\n",
        "  #print(termprobabilities)\n",
        "  total = 0\n",
        "  for elements in range(0, len(termprobabilities)): \n",
        "    total += termprobabilities[elements] \n",
        "  total_probability = probClass + total\n",
        "  #print(probClass)\n",
        "  #print(total)\n",
        "  return total_probability\n",
        "\n",
        "hamProbLog = probabilityClassification(termsTestHam, probabilityHam, termsHam)\n",
        "spamProbLog = probabilityClassification(termsTestSpam, probabilitySpam, termsSpam)\n",
        "\n",
        "print(hamProbLog, spamProbLog)\n",
        "\n",
        "def finalClassification(ham, spam):\n",
        "  if spam > ham:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "classification = finalClassification(hamProbLog,spamProbLog)\n",
        "\n",
        "print(classification)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-11293.413990933324 -18965.072892477325\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZB_f_OiwjmS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he6_n5adLUVF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "7bfcb7bf-210b-479d-f284-0704ab370df9"
      },
      "source": [
        "#accuracy algorithm\n",
        "\n",
        "nCorrect = 0\n",
        "nIncorrect = 0\n",
        "    \n",
        "for file in dfsTestHam:\n",
        "  testHamWords = getWords2(file)\n",
        "  log_prob_file_ham = probabilityClassification(testHamWords, probabilityHam, termsHam)\n",
        "  log_prob_file_spam = probabilityClassification(testHamWords,probabilitySpam, termsSpam)\n",
        "  final_class = finalClassification(log_prob_file_ham,log_prob_file_spam)\n",
        "  #print(final_class)\n",
        "  if final_class == 0:\n",
        "    nCorrect += 1\n",
        "  else:\n",
        "    nIncorrect += 1\n",
        "\n",
        "for file in dfsTestSpam:\n",
        "  testSpamWords = getWords2(file)\n",
        "  log_prob_file_ham1 = probabilityClassification(testSpamWords, probabilityHam, termsHam)\n",
        "  log_prob_file_spam1 = probabilityClassification(testSpamWords,probabilitySpam, termsSpam)\n",
        "  final_class1 = finalClassification(log_prob_file_ham1,log_prob_file_spam1)\n",
        "  #print(final_class1)\n",
        "  if final_class1 == 1:\n",
        "    nCorrect += 1\n",
        "  else:\n",
        "    nIncorrect += 1\n",
        "\n",
        "\n",
        "\n",
        "final_accuracy = nCorrect / (nCorrect + nIncorrect) * 100\n",
        "\n",
        "\n",
        "print('Final Accuracy for Data Set 1:')\n",
        "print(final_accuracy)\n",
        "\n",
        "\n",
        "def accuracy(listTestHam, listTestSpam, probabHam, probabSpam, termsHam_, termsSpam_):\n",
        "  numCorrect = 0\n",
        "  numIncorrect = 0\n",
        "    \n",
        "  for file in listTestHam:\n",
        "    testHamWords = getWords2(file)\n",
        "    log_prob_file_ham = probabilityClassification(testHamWords, probabHam, termsHam_)\n",
        "    log_prob_file_spam = probabilityClassification(testHamWords,probabSpam, termsSpam_)\n",
        "    final_class = finalClassification(log_prob_file_ham,log_prob_file_spam)\n",
        "    #print(final_class)\n",
        "    if final_class == 0:\n",
        "      numCorrect += 1\n",
        "    else:\n",
        "      numIncorrect += 1\n",
        "\n",
        "  for file in listTestSpam:\n",
        "    testSpamWords = getWords2(file)\n",
        "    log_prob_file_ham1 = probabilityClassification(testSpamWords, probabHam, termsHam_)\n",
        "    log_prob_file_spam1 = probabilityClassification(testSpamWords,probabSpam, termsSpam_)\n",
        "    final_class1 = finalClassification(log_prob_file_ham1,log_prob_file_spam1)\n",
        "    #print(final_class1)\n",
        "    if final_class1 == 1:\n",
        "      numCorrect += 1\n",
        "    else:\n",
        "      numIncorrect += 1\n",
        "\n",
        "\n",
        "\n",
        "  final_acc = numCorrect / (numCorrect + numIncorrect) * 100\n",
        "  return final_acc\n",
        "\n",
        "accuracy_data_2 = accuracy(dfsTestHamData2,dfsTestSpamData2,probHamData2,probSpamData2,termsHamData2,termsSpamData2)\n",
        "accuracy_data_3 = accuracy(dfsTestHamData3,dfsTestSpamData3,probHamData3,probSpamData3,termsHamData3,termsSpamData3)\n",
        "  \n",
        "print('Final Accuracy for Data Set 2:')\n",
        "print(accuracy_data_2)\n",
        "\n",
        "print('Final Accuracy for Data Set 3:')\n",
        "print(accuracy_data_3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Accuracy for Data Set 1:\n",
            "94.35146443514645\n",
            "Final Accuracy for Data Set 2:\n",
            "94.51754385964912\n",
            "Final Accuracy for Data Set 3:\n",
            "81.57894736842105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_dRlg-PxKoO"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWrzB8xqjHq8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b6528f52-1822-4179-ca2a-5aacd358355f"
      },
      "source": [
        "\n",
        "#variables not yet defined: weight, data, partial_deriv_loss, lam\n",
        "\n",
        "\n",
        "\n",
        "#formula for when you wanna classify spam (P=1)\n",
        "def log_formula(h):\n",
        "  #print(1/(1+np.exp(-1*h)))\n",
        "  return 1/(1+np.exp(-1*h))\n",
        "\n",
        "def log_formula_zero(h):\n",
        "  return np.exp(-h)/(1+np.exp(-h))\n",
        "\n",
        "def h_function(weights, x_matrix):\n",
        "  return weights[0] + weights[1]*x_matrix.iloc[:,1] + weights[2]*x_matrix.iloc[:,2]\n",
        "\n",
        "#formula for loss function\n",
        "#yPredY_One = output of log_formula\n",
        "#yPredY_Zero = output of log_formula_zero\n",
        "#y = pHam\n",
        "#one_minus_y = pSpam\n",
        "def loss_function(weights,x_matrix,probabHam):\n",
        "  y_predicted = h_function(weights,x_matrix)\n",
        "  check = probabHam * np.log10(y_predicted) + (1-probabHam) * np.log10(1-y_predicted)\n",
        "  check = check.replace([-np.inf], -100)\n",
        "  check = check.replace([np.inf], 100)\n",
        "  #print(check)\n",
        "  return -sum(check)\n",
        "\n",
        "#formula for gradient ascent (still needs to be done)\n",
        "def gradient(weight, h_funct, probabHam, df_X):\n",
        "  g = [0,0,0]\n",
        "  y_pred = log_formula(h_funct)\n",
        "  g[0] = -1 * sum(probabHam*(y_pred) - (1-probabHam) * (1-y_pred))\n",
        "  g[1] = -1 * sum(probabHam*(y_pred)*df_X.iloc[:,1] - (1-probabHam)*(1-y_pred)*df_X.iloc[:,1])\n",
        "  g[2] = -1 * sum(probabHam*(y_pred)*df_X.iloc[:,2] - (1-probabHam)*(1-y_pred)*df_X.iloc[:,2])\n",
        "  return g\n",
        "\n",
        "def ascent(w_new,w_prev,lr,x_matrix):\n",
        "  j=0\n",
        "  while True:\n",
        "    w_prev = w_new\n",
        "    \n",
        "    w0 = w_prev[0] + lr*gradient(w_prev,h_function(w_prev,x_matrix),pHam,x_matrix)[0]\n",
        "    w1 = w_prev[1] + lr*gradient(w_prev,h_function(w_prev,x_matrix),pHam,x_matrix)[1]\n",
        "    w2 = w_prev[2] + lr*gradient(w_prev,h_function(w_prev,x_matrix),pHam,x_matrix)[2]\n",
        "    w_new = [w0,w1,w2]\n",
        "    if((w_new[0] - w_prev[0])**2 + (w_new[1] - w_prev[1])**2 + (w_new[2] - w_prev[2])**2 < pow(10,6)):\n",
        "      return w_new\n",
        "    if j > 100:\n",
        "      return w_new\n",
        "    j+=1\n",
        "\n",
        "#initialize weights, will be updated with every call of the gradient ascent function\n",
        "weight0 = 1\n",
        "weight_ham = 1\n",
        "weight_spam = 1\n",
        "weight = [1,1,1]\n",
        "\n",
        "\n",
        "#stores the words contained in all ham and spam files for training data\n",
        "all_words = []\n",
        "\n",
        "#appends words in ham training data\n",
        "for word in termsHam:\n",
        "  all_words.append(word)\n",
        "#appends words in spam training data\n",
        "for word in termsSpam:\n",
        "  all_words.append(word)\n",
        "\n",
        "#part of converting all_words into a data frame\n",
        "all_words = list(dict.fromkeys(all_words))\n",
        "#print(all_words)\n",
        "\n",
        "#converts all_words into data frame\n",
        "all_words_df = pd.DataFrame(all_words, columns = ['words'])\n",
        "#print(all_words_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#part of combining the ham and spam training file words into a data frame\n",
        "train_ham_df = pd.DataFrame(list(termsHam.items()), columns = ['words','frequency_x0'])\n",
        "train_spam_df = pd.DataFrame(list(termsSpam.items()), columns = ['words', 'frequency_x1'])\n",
        "\n",
        "check_df = all_words_df.merge(train_ham_df, how='left', left_on='words', right_on='words')\n",
        "check_df = check_df.merge(train_spam_df, how='left', left_on='words', right_on='words')\n",
        "\n",
        "\n",
        "check_df = check_df.fillna(0)\n",
        "\n",
        "weight = ascent(weight,weight,.0099,check_df)\n",
        "\n",
        "print(weight)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-33.77977573681067, -218.80786890693085, -88.56349250177685]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRbaZOU08uzU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "951e9b80-c068-4fc4-f450-a60877d3e013"
      },
      "source": [
        "def accuracy_log(listTestHam, listTestSpam, probabHam, probabSpam, termsHam_, termsSpam_):\n",
        "  numCorrect = 0\n",
        "  numIncorrect = 0\n",
        "    \n",
        "  for file in listTestHam:\n",
        "    testHamWords = getWords2(file)\n",
        "    word_list = []\n",
        "\n",
        "    #appends words in ham training data\n",
        "    for word in testHamWords:\n",
        "      word_list.append(word)\n",
        "    #appends words in spam training data\n",
        "    for word in testHamWords:\n",
        "      word_list.append(word)\n",
        "\n",
        "    #part of converting all_words into a data frame\n",
        "    word_list = list(dict.fromkeys(word_list))\n",
        "    #print(all_words)\n",
        "\n",
        "    #converts all_words into data frame\n",
        "    word_list_df = pd.DataFrame(word_list, columns = ['words'])\n",
        "    #print(all_words_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #part of combining the ham and spam training file words into a data frame\n",
        "    test_ham_df = pd.DataFrame(list(word_list_df.items()), columns = ['words','frequency_x0'])\n",
        "    test_spam_df = pd.DataFrame(list(word_list_df.items()), columns = ['words', 'frequency_x1'])\n",
        "\n",
        "    test_df = all_words_df.merge(test_ham_df, how='left', left_on='words', right_on='words')\n",
        "    test_df = check_df.merge(test_spam_df, how='left', left_on='words', right_on='words')\n",
        "\n",
        "    test_df = test_df.fillna(0)\n",
        "    \n",
        "    #def log_formula(h):\n",
        "    #  print(1/(1+np.exp(-1*h)))\n",
        "    #  return 1/(1+np.exp(-1*h))\n",
        "\n",
        "    #def h_function(weights, x_matrix):\n",
        "\n",
        "    h_var = h_function(weight, test_df)\n",
        "    file_classification_ham = log_formula(h_var)\n",
        "    file_classification_spam = log_formula_zero(h_var)\n",
        "\n",
        "    classification_ham_sum = file_classification_ham.sum()\n",
        "    classification_spam_sum = file_classification_spam.sum()\n",
        "\n",
        "    #print(classification_ham_sum)\n",
        "    #print(classification_spam_sum)\n",
        "    final_class = finalClassification(classification_ham_sum,classification_spam_sum)\n",
        "    #print(final_class)\n",
        "    if final_class == 1:\n",
        "      numCorrect += 1\n",
        "    else:\n",
        "      numIncorrect += 1\n",
        "  '''\n",
        "  for file in listTestSpam:\n",
        "    testSpamWords = getWords2(file)\n",
        "    log_prob_file_ham1 = probabilityClassification(testSpamWords, probabHam, termsHam_)\n",
        "    log_prob_file_spam1 = probabilityClassification(testSpamWords,probabSpam, termsSpam_)\n",
        "    final_class1 = finalClassification(log_prob_file_ham1,log_prob_file_spam1)\n",
        "    #print(final_class1)\n",
        "    if final_class1 == 1:\n",
        "      numCorrect += 1\n",
        "    else:\n",
        "      numIncorrect += 1\n",
        "  '''\n",
        "  for file in listTestSpam:\n",
        "    testSpamWords = getWords2(file)\n",
        "    word_list = []\n",
        "\n",
        "    #appends words in ham training data\n",
        "    for word in testSpamWords:\n",
        "      word_list.append(word)\n",
        "    #appends words in spam training data\n",
        "    for word in testSpamWords:\n",
        "      word_list.append(word)\n",
        "\n",
        "    #part of converting all_words into a data frame\n",
        "    word_list = list(dict.fromkeys(word_list))\n",
        "    #print(all_words)\n",
        "\n",
        "    #converts all_words into data frame\n",
        "    word_list_df = pd.DataFrame(word_list, columns = ['words'])\n",
        "    #print(all_words_df)\n",
        "\n",
        "    #part of combining the ham and spam training file words into a data frame\n",
        "    test_ham_df = pd.DataFrame(list(word_list_df.items()), columns = ['words','frequency_x0'])\n",
        "    test_spam_df = pd.DataFrame(list(word_list_df.items()), columns = ['words', 'frequency_x1'])\n",
        "\n",
        "    test_df = all_words_df.merge(test_ham_df, how='left', left_on='words', right_on='words')\n",
        "    test_df = check_df.merge(test_spam_df, how='left', left_on='words', right_on='words')\n",
        "\n",
        "    test_df = test_df.fillna(0)\n",
        "    \n",
        "    #def log_formula(h):\n",
        "    #  print(1/(1+np.exp(-1*h)))\n",
        "    #  return 1/(1+np.exp(-1*h))\n",
        "\n",
        "    #def h_function(weights, x_matrix):\n",
        "\n",
        "    h_var = h_function(weight, test_df)\n",
        "    file_classification_ham = log_formula(h_var)\n",
        "    file_classification_spam = log_formula_zero(h_var)\n",
        "\n",
        "    classification_ham_sum = file_classification_ham.sum()\n",
        "    classification_spam_sum = file_classification_spam.sum()\n",
        "\n",
        "    #print(classification_ham_sum)\n",
        "    #print(classification_spam_sum)\n",
        "    final_class = finalClassification(classification_ham_sum,classification_spam_sum)\n",
        "    #print(final_class)\n",
        "    if final_class == 0:\n",
        "      numCorrect += 1\n",
        "    else:\n",
        "      numIncorrect += 1\n",
        "\n",
        "\n",
        "  final_acc = numCorrect / (numCorrect + numIncorrect) * 100\n",
        "  return final_acc\n",
        "\n",
        "accuracy_data_log = accuracy_log(dfsTestHam,dfsTestSpam,pHam,pSpam,termsHam,termsSpam)\n",
        "print(\"Accuracy for Data Set 1:\")\n",
        "print(accuracy_data_log)\n",
        "accuracy_data_2_log = accuracy_log(dfsTestHamData2,dfsTestSpamData2,probHamData2,probSpamData2,termsHamData2,termsSpamData2)\n",
        "print(\"Accuracy for Data Set 2:\")\n",
        "print(accuracy_data_2_log)\n",
        "accuracy_data_3_log = accuracy_log(dfsTestHamData3,dfsTestSpamData3,probHamData3,probSpamData3,termsHamData3,termsSpamData3)\n",
        "print(\"Accuracy for Data Set 3:\")\n",
        "print(accuracy_data_3_log)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: overflow encountered in exp\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy for Data Set 1:\n",
            "72.80334728033473\n",
            "Accuracy for Data Set 2:\n",
            "67.32456140350878\n",
            "Accuracy for Data Set 3:\n",
            "67.32456140350878\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}